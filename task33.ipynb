{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  csv\n",
    "import  rdflib\n",
    "from    rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, URIRef\n",
    "from    rdflib.namespace import FOAF, DCTERMS, XSD, RDF, SDO, RDFS\n",
    "\n",
    "g = Graph()\n",
    "foodnutrition = Namespace('http://kg-course/foodnutrition/')\n",
    "# Define namespaces\n",
    "g.bind(\"foodnutrition\", foodnutrition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Add Classes and Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N59b6f3c8a6fb4b8b9fab0ccf804b8bef (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add Classes\n",
    "g.add((SDO.Recipe, RDFS.subClassOf, OWL.Thing))\n",
    "g.add((SDO.Restaurant, RDFS.subClassOf, OWL.Thing))\n",
    "g.add((SDO.NutritionInformation, RDFS.subClassOf, OWL.Thing))\n",
    "g.add((SDO.Review, RDFS.subClassOf, OWL.Thing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipe Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Recipe Properties\n",
    "g.add((SDO.identifier, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.name, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.cookTime, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.prepTime, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.datePublished, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.image, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.recipeCategory, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.keywords, RDFS.domain, SDO.Recipe))\n",
    "# #g.add((SDO.name, RDFS.domain, SDO.Recipe)) QUANTITIES\n",
    "# #g.add((SDO.name, RDFS.domain, SDO.Recipe)) PARTS\n",
    "# g.add((SDO.yield, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.recipeYield, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.recipeInstructions, RDFS.domain, SDO.Recipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutrition Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Nutrition Properties\n",
    "g.add((SDO.name, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.description, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.recipeCategory, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.calories, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.fatContent, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.saturatedFatContent, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.cholesterolContent, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.sodiumContent, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.carbohydrateContent, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.fiberContent, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.sugarContent, RDFS.domain, SDO.NutritionInformation))\n",
    "g.add((SDO.proteinContent, RDFS.domain, SDO.NutritionInformation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N59b6f3c8a6fb4b8b9fab0ccf804b8bef (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Review Properties\n",
    "reviewId = foodnutrition[\"reviewId\"]\n",
    "g.add((reviewId, RDFS.domain, SDO.Review))\n",
    "g.add((reviewId, RDFS.range, RDFS.Literal))\n",
    "\n",
    "recipeID = foodnutrition[\"recipeID\"]\n",
    "g.add((recipeID, RDFS.domain, SDO.Review))\n",
    "g.add((recipeID, RDFS.range, RDFS.Literal))\n",
    "\n",
    "authorId = foodnutrition[\"authorId\"]\n",
    "g.add((authorId, RDFS.domain, SDO.Review))\n",
    "g.add((authorId, RDFS.range, RDFS.Literal))\n",
    "\n",
    "g.add((SDO.author, RDFS.domain, SDO.Review))\n",
    "g.add((SDO.reviewBody, RDFS.domain, SDO.Review))\n",
    "g.add((SDO.datePublished, RDFS.domain, SDO.Review))\n",
    "g.add((SDO.dateModified, RDFS.domain, SDO.Review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restaurant Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Resuturant Properties\n",
    "g.add((SDO.identifier, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.name, RDFS.domain, SDO.Restaurant))\n",
    "\n",
    "#expand location\n",
    "g.add((SDO.location, RDFS.domain, SDO.Restaurant))\n",
    "\n",
    "g.add((SDO.servesCuisine, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.currenciesAccepted, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.acceptsReservations, RDFS.domain, SDO.Restaurant))\n",
    "\n",
    "#3 wierd properties\n",
    "\n",
    "g.add((SDO.priceRange, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.aggregateRating, RDFS.domain, SDO.Restaurant))\n",
    "\n",
    "# 2 wierd Properties\n",
    "\n",
    "votes = foodnutrition[\"votes\"]\n",
    "g.add((votes, RDFS.subClassOf, RDF.Property))\n",
    "g.add((votes, RDFS.domain, SDO.Restaurant))\n",
    "g.add((votes, RDFS.range, RDFS.Literal))\n",
    "\n",
    "g.add((SDO.priceRange, RDFS.domain, SDO.Restaurant))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add data to our KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutrition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Nutrition.csv\", mode='r', newline='', encoding='latin-1') as file:\n",
    "    reader = csv.reader(file, delimiter=';')  # Create a reader object\n",
    "    header = next(reader)  # Read the header row\n",
    "    print(\"Header:\", header)\n",
    "    i = 0\n",
    "\n",
    "    # Read and print each row\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        #print(row[1])\n",
    "        #TODO: make better ID than i\n",
    "        Nutrition = foodnutrition[\"Nutrition/\" + str(i)]\n",
    "        g.add((Nutrition, RDF.type, SDO.NutritionInformation))\n",
    "        g.set((Nutrition, SDO.name, Literal(row[0], lang='en')))\n",
    "        g.set((Nutrition, SDO.description, Literal(row[1], lang='en')))\n",
    "        g.set((Nutrition, SDO.recipeCategory, Literal(row[2])))\n",
    "        g.set((Nutrition, SDO.calories, Literal(row[3])))\n",
    "        g.set((Nutrition, SDO.fatContent, Literal(row[4])))\n",
    "        g.set((Nutrition, SDO.saturatedFatContent, Literal(row[5])))\n",
    "        g.set((Nutrition, SDO.cholesterolContent, Literal(row[6])))\n",
    "        g.set((Nutrition, SDO.sodiumContent, Literal(row[7])))\n",
    "        g.set((Nutrition, SDO.carbohydrateContent, Literal(row[8])))\n",
    "        g.set((Nutrition, SDO.fiberContent, Literal(row[9])))\n",
    "        g.set((Nutrition, SDO.sugarContent, Literal(row[10])))\n",
    "        g.set((Nutrition, SDO.proteinContent, Literal(row[11])))\n",
    "        \n",
    "        if i == 10: break\n",
    "g.serialize('top10Nutrituion.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ORIGINAL, without quant and parts\n",
    "# with open(\"Recipes.csv\", mode='r', newline='', encoding='latin-1') as file:\n",
    "#     reader = csv.reader(file, delimiter=';')  # Create a reader object\n",
    "#     header = next(reader)  # Read the header row\n",
    "#     print(\"Header:\", header)\n",
    "#     i = 0\n",
    "\n",
    "#     # Read and print each row\n",
    "#     for row in reader:\n",
    "#         i += 1\n",
    "#         Recipe_uri = foodnutrition[\"Recipe/\" + row[0]]\n",
    "#         g.add((Recipe_uri, RDF.type, SDO.Recipe))\n",
    "#         g.set((Recipe_uri, SDO.name, Literal(row[1], lang='en')))\n",
    "#         g.set((Recipe_uri, SDO.cookTime, Literal(row[2])))\n",
    "#         g.set((Recipe_uri, SDO.prepTime, Literal(row[3])))\n",
    "#         g.set((Recipe_uri, SDO.datePublished, Literal(row[4], datatype=XSD.date)))\n",
    "#         g.set((Recipe_uri, SDO.image, Literal(row[5])))\n",
    "#         g.set((Recipe_uri, SDO.recipeCategory, Literal(row[6])))\n",
    "#         g.set((Recipe_uri, SDO.keywords, Literal(row[7])))\n",
    "#         #Quantities - row[8] \n",
    "#         #Parts - row[9]\n",
    "#         g.set((Recipe_uri, SDO.recipeYield, Literal(row[10]))) # yield\n",
    "#         #Recipe yield - row[11] - Don't need now\n",
    "#         g.set((Recipe_uri, SDO.recipeInstructions, Literal(row[12])))\n",
    "        \n",
    "#         if i == 10: break\n",
    "# print(g.serialize('top10Recipes.ttl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "\n",
    "# Recipe properties\n",
    "g.add((SDO.hasPart, RDF.type, RDF.Property))\n",
    "g.add((SDO.hasPart, RDFS.domain, SDO.Recipe))          # Recipes have parts (ingredients)\n",
    "g.add((SDO.hasPart, RDFS.range, SDO.HowToItem))        # The parts are HowToItem (ingredients)\n",
    "\n",
    "# Ingredient (HowToItem) properties\n",
    "g.add((SDO.name, RDF.type, RDF.Property))\n",
    "g.add((SDO.name, RDFS.domain, SDO.HowToItem))          # Ingredients have names\n",
    "g.add((SDO.amount, RDF.type, RDF.Property))\n",
    "g.add((SDO.amount, RDFS.domain, SDO.HowToItem))        # Ingredients have amounts/quantities\n",
    "g.add((SDO.amount, RDFS.range, XSD.string))            # Amount is represented as a string (e.g., \"1/2 cup\")\n",
    "\n",
    "# Open the CSV file\n",
    "with open(\"Recipes.csv\", mode='r', newline='', encoding='latin-1') as file:\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    header = next(reader)  # Read the header row\n",
    "    print(\"Header:\", header)\n",
    "    \n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        Recipe_uri = foodnutrition[\"Recipe/\" + row[0]]  # Create unique URI for each recipe        \n",
    "        # Add basic recipe properties\n",
    "        g.add((Recipe_uri, RDF.type, SDO.Recipe))\n",
    "        g.set((Recipe_uri, SDO.name, Literal(row[1], lang='en')))\n",
    "        g.set((Recipe_uri, SDO.cookTime, Literal(row[2])))\n",
    "        g.set((Recipe_uri, SDO.prepTime, Literal(row[3])))\n",
    "        g.set((Recipe_uri, SDO.datePublished, Literal(row[4], datatype=XSD.date)))\n",
    "        g.set((Recipe_uri, SDO.image, Literal(row[5])))\n",
    "        g.set((Recipe_uri, SDO.recipeCategory, Literal(row[6])))\n",
    "        g.set((Recipe_uri, SDO.keywords, Literal(row[7])))\n",
    "        g.set((Recipe_uri, SDO.recipeYield, Literal(row[10])))\n",
    "        g.set((Recipe_uri, SDO.recipeInstructions, Literal(row[12])))\n",
    "\n",
    "        # --- Handle Quantities and Parts ---\n",
    "        quantities_raw = row[8]  # Quantities (e.g., '1/2,5,2,1,1,1')\n",
    "        parts_raw = row[9]       # Ingredients (e.g., 'rice vinegar,haeo')\n",
    "\n",
    "        # Convert to lists, clean 'c()' and whitespace\n",
    "        quantities = [q.strip().strip('\"') for q in quantities_raw.strip('c()').split(',') if q.strip()]\n",
    "        parts = [p.strip().strip('\"') for p in parts_raw.strip('c()').split(',') if p.strip()]\n",
    "\n",
    "        # Match quantities with parts, handling mismatches\n",
    "        max_len = max(len(quantities), len(parts))\n",
    "        \n",
    "        for idx in range(max_len):\n",
    "            quantity = quantities[idx] if idx < len(quantities) else None\n",
    "            part = parts[idx] if idx < len(parts) else None\n",
    "            \n",
    "            if not part or part.lower() == 'na':  # Skip if part is missing or 'NA'\n",
    "                continue\n",
    "\n",
    "            # Create a unique URI for each ingredient\n",
    "            ingredient_uri = foodnutrition[f\"Recipe/{row[0]}/Ingredient/{idx+1}\"]\n",
    "\n",
    "            # Add ingredient as HowToItem\n",
    "            g.add((ingredient_uri, RDF.type, SDO.HowToItem))\n",
    "            g.add((ingredient_uri, SDO.name, Literal(part, lang='en')))\n",
    "            \n",
    "            if quantity and quantity.lower() != 'na':\n",
    "                g.add((ingredient_uri, SDO.amount, Literal(quantity)))\n",
    "\n",
    "            # Link ingredient to the recipe\n",
    "            g.add((Recipe_uri, SDO.hasPart, ingredient_uri))\n",
    "\n",
    "        if i == 10:  # Limit to first 10 recipes\n",
    "            break\n",
    "\n",
    "# Serialize to Turtle format\n",
    "print(g.serialize('top10Recipes_test.ttl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import zip_longest\n",
    "from rdflib import URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "\n",
    "# Recipe properties\n",
    "g.add((SDO.hasPart, RDF.type, RDF.Property))\n",
    "g.add((SDO.hasPart, RDFS.domain, SDO.Recipe))          # Recipes have parts (ingredients)\n",
    "g.add((SDO.hasPart, RDFS.range, SDO.HowToItem))        # The parts are HowToItem (ingredients)\n",
    "\n",
    "# Ingredient (HowToItem) properties\n",
    "g.add((SDO.name, RDF.type, RDF.Property))\n",
    "g.add((SDO.name, RDFS.domain, SDO.HowToItem))          # Ingredients have names\n",
    "g.add((SDO.amount, RDF.type, RDF.Property))\n",
    "g.add((SDO.amount, RDFS.domain, SDO.HowToItem))        # Ingredients have amounts/quantities\n",
    "g.add((SDO.amount, RDFS.range, XSD.string))            # Amount is represented as a string (e.g., \"1/2 cup\")\n",
    "\n",
    "# Read the CSV and add data to the graph\n",
    "with open(\"Recipes.csv\", mode='r', newline='', encoding='latin-1') as file:\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    header = next(reader)\n",
    "    print(\"Header:\", header)\n",
    "\n",
    "    for i, row in enumerate(reader):\n",
    "        Recipe_uri = foodnutrition[\"Recipe/\" + row[0]]\n",
    "        g.add((Recipe_uri, RDF.type, SDO.Recipe))\n",
    "        g.set((Recipe_uri, SDO.name, Literal(row[1], lang='en')))\n",
    "        g.set((Recipe_uri, SDO.cookTime, Literal(row[2])))\n",
    "        g.set((Recipe_uri, SDO.prepTime, Literal(row[3])))\n",
    "        g.set((Recipe_uri, SDO.datePublished, Literal(row[4], datatype=XSD.date)))\n",
    "        g.set((Recipe_uri, SDO.image, Literal(row[5])))\n",
    "        g.set((Recipe_uri, SDO.recipeCategory, Literal(row[6])))\n",
    "        g.set((Recipe_uri, SDO.keywords, Literal(row[7])))\n",
    "        g.set((Recipe_uri, SDO.recipeYield, Literal(row[10])))\n",
    "        g.set((Recipe_uri, SDO.recipeInstructions, Literal(row[12])))\n",
    "\n",
    "        # Handle Quantities and Parts\n",
    "        quantities = row[8].strip('c()').replace('\"', '').split(',')  # Parse the column as a list\n",
    "        parts = row[9].strip('c()').replace('\"', '').split(',')\n",
    "\n",
    "        # Pair quantities and parts, handling mismatched lengths\n",
    "        for idx, (quantity, part) in enumerate(zip_longest(quantities, parts)):\n",
    "            if part is None:\n",
    "                continue  # Skip if there's no ingredient name\n",
    "            ingredient_uri = URIRef(f\"{Recipe_uri}/Ingredient/{idx+1}\")\n",
    "            g.add((Recipe_uri, SDO.hasPart, ingredient_uri))\n",
    "            g.add((ingredient_uri, RDF.type, SDO.HowToItem))\n",
    "            g.set((ingredient_uri, SDO.name, Literal(part, lang='en')))\n",
    "\n",
    "            if quantity is not None and quantity != \"NA\":\n",
    "                g.set((ingredient_uri, SDO.amount, Literal(quantity)))\n",
    "\n",
    "        if i == 10:  # Limit to first 10 recipes for testing\n",
    "            break\n",
    "\n",
    "# Serialize the graph\n",
    "print(g.serialize('top10Recipes_test2.ttl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restaurant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Restaurants.csv\", mode='r', newline='', encoding='latin-1') as file:\n",
    "    reader = csv.reader(file, delimiter=';')  # Create a reader object\n",
    "    header = next(reader)  # Read the header row\n",
    "    print(\"Header:\", header)\n",
    "    i = 0\n",
    "\n",
    "    # Read and print each row\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        Resturant = foodnutrition[\"Restaurant/\" + row[0]]\n",
    "        g.add((Resturant, RDF.type, SDO.Restaurant))\n",
    "        g.set((Resturant, SDO.name, Literal(row[1], lang='en')))\n",
    "\n",
    "        g.set((Resturant, SDO.servesCuisine, Literal(row[8], lang='en')))\n",
    "        g.set((Resturant, SDO.currenciesAccepted, Literal(row[9], lang='en')))\n",
    "        g.set((Resturant, SDO.acceptsReservations, Literal(row[10], datatype=XSD.boolean)))\n",
    "        g.set((Resturant, SDO.priceRange, Literal(row[14])))\n",
    "        g.set((Resturant, SDO.aggregateRating, Literal(row[15])))\n",
    "        g.set((Resturant, foodnutrition.votes, Literal(row[18])))\n",
    "\n",
    "        g.set((Resturant, SDO.priceRange, Literal(row[20])))\n",
    " \n",
    "        \n",
    "        if i == 10: break\n",
    "print(g.serialize('top10Restaurants.ttl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews data (added at task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Reviews.txt\", mode='r', newline='', encoding='latin-1') as file:\n",
    "#     reader = csv.reader(file, delimiter='\\t')  # Create a reader object\n",
    "#     header = next(reader)  # Read the header row\n",
    "#     print(\"Header:\", header)\n",
    "#     i = 0\n",
    "\n",
    "#     # Read and print each row\n",
    "#     for row in reader:\n",
    "#         i += 1\n",
    "#         #print(row[1])\n",
    "#         Review = foodnutrition[\"Review\" + row[0]]\n",
    "#         g.add((Review, RDF.type, SDO.Review))\n",
    "#         g.set((Review, foodnutrition.reviewId, Literal(row[1])))\n",
    "#         g.set((Review, foodnutrition.recipeId, Literal(row[2])))\n",
    "#         g.set((Review, foodnutrition.authorId, Literal(row[3])))\n",
    "#         g.set((Review, SDO.author, Literal(row[2])))\n",
    "#         g.set((Review, SDO.reviewBody, Literal(row[4])))\n",
    "#         g.set((Review, SDO.datePublished, Literal(row[5], datatype=XSD.date)))\n",
    "#         g.set((Review, SDO.dateModified, Literal(row[6], datatype=XSD.date)))\n",
    "\n",
    "                \n",
    "#         if i == 10: break\n",
    "\n",
    "# print(g.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 NLP extract ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean Reviews.txt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the entire file as raw text\n",
    "with open('Reviews_test.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Replace triple quotes and newlines within reviews\n",
    "# Keep newlines that separate proper reviews (i.e., before ReviewId)\n",
    "cleaned_text = re.sub(r'(\"\"\"\"|\\n)(?!(\\d+\\t))', ' ', raw_text)\n",
    "\n",
    "# Now cleaned_text should have each review on a single line, except for proper row separations\n",
    "from io import StringIO\n",
    "\n",
    "# Use StringIO to treat the cleaned text like a file\n",
    "cleaned_file = StringIO(cleaned_text)\n",
    "\n",
    "# Load into pandas DataFrame\n",
    "rev_df = pd.read_csv(cleaned_file, delimiter='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\Desktop\\school\\cursuri\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ingredients saved to 20250213_181954_extracted_ingredients.csv\n",
      "Ingredient counts saved to 20250213_181954_ingredient_counts.csv\n"
     ]
    }
   ],
   "source": [
    "# #### With csv for testing\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "# from datetime import datetime\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "# import csv\n",
    "\n",
    "# # rev_df = pd.read_csv('path_to_your_cleaned_file.csv')  # Uncomment if you need to reload\n",
    "\n",
    "# # NER model for food\n",
    "# tokenizer_ner = AutoTokenizer.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")\n",
    "# model_ner = AutoModelForTokenClassification.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\").to(\"cuda\")\n",
    "# ner_pipe = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, device=0) # device=-1 for CPU\n",
    "\n",
    "# # Function to convert entities to a list\n",
    "# def convert_entities_to_list(text, entities):\n",
    "#     ents = []\n",
    "#     for ent in entities:\n",
    "#         e = {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "#         if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
    "#             ents[-1][\"end\"] = e[\"end\"]\n",
    "#             continue\n",
    "#         ents.append(e)\n",
    "#     return [text[e[\"start\"]:e[\"end\"]] for e in ents]\n",
    "\n",
    "# # Initialize storage for results\n",
    "# results = []\n",
    "# ingredient_counts = defaultdict(int)\n",
    "\n",
    "# # Process each review\n",
    "# for idx, row in rev_df.iterrows():\n",
    "#     review_text = row.get('Review', '')  # Ensure the column name matches your DataFrame\n",
    "    \n",
    "#     # Run NER model\n",
    "#     ner_entity_results = ner_pipe(review_text, aggregation_strategy=\"simple\")\n",
    "#     ingredients = convert_entities_to_list(review_text, ner_entity_results)\n",
    "\n",
    "#     # Update ingredient counts\n",
    "#     for ingredient in ingredients:\n",
    "#         ingredient_counts[ingredient] += 1\n",
    "\n",
    "#     # Store the results\n",
    "#     results.append({\n",
    "#         'Review': review_text,\n",
    "#         'Extracted Ingredients': ', '.join(ingredients) if ingredients else 'None'\n",
    "#     })\n",
    "\n",
    "# # Generate timestamps for unique filenames\n",
    "# current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# extracted_file = f\"{current_time}_extracted_ingredients.csv\"\n",
    "# counted_file = f\"{current_time}_ingredient_counts.csv\"\n",
    "\n",
    "\n",
    "# # Write extracted ingredients to a CSV file\n",
    "# with open(extracted_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     fieldnames = ['Review', 'Extracted Ingredients']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(results)\n",
    "\n",
    "# print(f\"Extracted ingredients saved to {extracted_file}\")\n",
    "\n",
    "# # Write ingredient counts to a separate CSV file\n",
    "# with open(counted_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     fieldnames = ['Ingredient', 'Count']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "#     for ingredient, count in sorted(ingredient_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "#         writer.writerow({'Ingredient': ingredient, 'Count': count})\n",
    "\n",
    "# print(f\"Ingredient counts saved to {counted_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NLP to extract ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with extracted ingredients saved to 20250213_230834_extracted_ingredients.csv\n",
      "Ingredient counts saved to 20250213_230834_ingredient_counts.csv\n"
     ]
    }
   ],
   "source": [
    "#### Without csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import csv\n",
    "\n",
    "# Load your DataFrame (assuming 'rev_df' is already prepared)\n",
    "# rev_df = pd.read_csv('path_to_your_cleaned_file.csv')  # Uncomment if needed\n",
    "\n",
    "# NER model for food\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\").to(\"cuda\")\n",
    "ner_pipe = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, device=0)  # device=-1 for CPU\n",
    "\n",
    "# Function to convert entities to a list\n",
    "def convert_entities_to_list(text, entities):\n",
    "    ents = []\n",
    "    for ent in entities:\n",
    "        e = {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "        if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
    "            ents[-1][\"end\"] = e[\"end\"]\n",
    "            continue\n",
    "        ents.append(e)\n",
    "    return [text[e[\"start\"]:e[\"end\"]] for e in ents]\n",
    "\n",
    "# Initialize storage for ingredient counts\n",
    "ingredient_counts = defaultdict(int)\n",
    "ingredients_list = []  # Store extracted ingredients for DataFrame\n",
    "\n",
    "# Process each review\n",
    "for idx, row in rev_df.iterrows():\n",
    "    review_text = row.get('Review', '')  # Ensure the column name matches your DataFrame\n",
    "\n",
    "    # Run NER model\n",
    "    ner_entity_results = ner_pipe(review_text, aggregation_strategy=\"simple\")\n",
    "    ingredients = convert_entities_to_list(review_text, ner_entity_results)\n",
    "\n",
    "    # Update ingredient counts\n",
    "    for ingredient in ingredients:\n",
    "        ingredient_counts[ingredient] += 1\n",
    "\n",
    "    # Store extracted ingredients for DataFrame\n",
    "    ingredients_list.append(', '.join(ingredients) if ingredients else 'None')\n",
    "\n",
    "# Add new \"Ingredients\" column to rev_df\n",
    "rev_df['Ingredients'] = ingredients_list\n",
    "\n",
    "# Generate timestamps for unique filenames\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "extracted_file = f\"{current_time}_extracted_ingredients.csv\"\n",
    "counted_file = f\"{current_time}_ingredient_counts.csv\"\n",
    "\n",
    "# Save updated DataFrame to CSV (optional)\n",
    "rev_df.to_csv(extracted_file, index=False, encoding='utf-8')\n",
    "print(f\"Updated DataFrame with extracted ingredients saved to {extracted_file}\")\n",
    "\n",
    "# Write ingredient counts to a separate CSV file\n",
    "with open(counted_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Ingredient', 'Count']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for ingredient, count in sorted(ingredient_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        writer.writerow({'Ingredient': ingredient, 'Count': count})\n",
    "\n",
    "print(f\"Ingredient counts saved to {counted_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add extracted ingredients to our KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients successfully added to the Knowledge Graph!\n"
     ]
    }
   ],
   "source": [
    "## Add ingredients to Review class\n",
    "# Add Review Properties for extracted ingredients\n",
    "extr_ingred = foodnutrition[\"extractedIngredients\"]\n",
    "\n",
    "g.add((extr_ingred, RDFS.domain, SDO.Review))\n",
    "g.add((extr_ingred, RDFS.range, RDFS.Literal))\n",
    "\n",
    "\n",
    "for idx, row in rev_df.iterrows():\n",
    "    review_uri = foodnutrition[f\"review_{row['ReviewId']}\"]  # Unique review identifier\n",
    "    g.add((review_uri, RDF.type, SDO.Review))  # Add review instance\n",
    "\n",
    "    # Iterate over individual ingredients and add separately\n",
    "    for ingredient in row[\"Ingredients\"].split(\", \"):  # Assuming ingredients are comma-separated\n",
    "        g.add((review_uri, extr_ingred, Literal(ingredient)))  # Add each ingredient as a separate triple\n",
    "\n",
    "\n",
    "# Save the updated KG to a file (optional)\n",
    "g.serialize(\"updated_kg.ttl\", format=\"turtle\")\n",
    "\n",
    "print(\"Ingredients added to the Knowledge Graph!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 NLP Sentiment analysis on Reviews.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the entire file as raw text\n",
    "with open('Reviews_test.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "import re\n",
    "\n",
    "# Replace triple quotes and newlines within reviews\n",
    "# Keep newlines that separate proper reviews (i.e., before ReviewId)\n",
    "cleaned_text = re.sub(r'(\"\"\"\"|\\n)(?!(\\d+\\t))', ' ', raw_text)\n",
    "\n",
    "# Now cleaned_text should have each review on a single line, except for proper row separations\n",
    "from io import StringIO\n",
    "\n",
    "# Use StringIO to treat the cleaned text like a file\n",
    "cleaned_file = StringIO(cleaned_text)\n",
    "\n",
    "# Load into pandas DataFrame\n",
    "rev_df = pd.read_csv(cleaned_file, delimiter='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP extract sentiment and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load Reviews.txt (assuming tab-separated values)\n",
    "# reviews_df = pd.read_csv('Reviews_test.txt', delimiter='\\t')\n",
    "reviews_df = rev_df\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline and use cuda device\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=0)\n",
    "# sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"facebook/bart-large-mnli\", device=0) # for neutral sentiment\n",
    "\n",
    "# Handle NaN values in the 'Review' column\n",
    "reviews_df['Review'] = reviews_df['Review'].fillna(\"\")\n",
    "\n",
    "# Apply sentiment analysis to the reviews\n",
    "sentiment_results = sentiment_pipeline(reviews_df['Review'].tolist())\n",
    "\n",
    "# Add sentiment results back to DataFrame\n",
    "reviews_df['Sentiment'] = [result['label'] for result in sentiment_results]\n",
    "reviews_df['Confidence'] = [result['score'] for result in sentiment_results]\n",
    "\n",
    "# Preview the results\n",
    "# print(reviews_df[['ReviewId', 'Review', 'Sentiment', 'Confidence']])\n",
    "\n",
    "# # Save the results to a CSV file\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"{current_time}_extracted_ingredients.csv\"\n",
    "reviews_df[['ReviewId', 'RecipeId', 'Review', 'Sentiment', 'Confidence']].to_csv(output_file, index=False)\n",
    "\n",
    "# print(\"Sentiment analysis results saved to 'sentiment_analysis_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data from Reviews to our KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment and confidence properties for the Review class\n",
    "sentiment = foodnutrition[\"sentiment\"]\n",
    "confidence = foodnutrition[\"confidence\"]\n",
    "\n",
    "g.add((sentiment, RDFS.domain, SDO.Review))\n",
    "g.add((sentiment, RDFS.range, RDFS.Literal))\n",
    "\n",
    "g.add((confidence, RDFS.domain, SDO.Review))\n",
    "g.add((confidence, RDFS.range, RDFS.Literal))\n",
    "\n",
    "\n",
    "i = 0\n",
    "# Loop through each review and add it to the KG\n",
    "for index, row in reviews_df.iterrows():\n",
    "    i += 1\n",
    "    \n",
    "    # Create a unique URI for each review\n",
    "    # review_uri = foodnutrition[\"Review/\" + str(i)]\n",
    "    review_uri = foodnutrition[f\"Review/{row['ReviewId']}\"]\n",
    "\n",
    "    # Add the Review type to the graph\n",
    "    g.add((review_uri, RDF.type, SDO.Review))\n",
    "    # Add properties to the Review\n",
    "    g.set((review_uri, foodnutrition.reviewId, Literal(row['ReviewId'])))\n",
    "    g.set((review_uri, foodnutrition.recipeID, Literal(row['RecipeId'])))\n",
    "    g.set((review_uri, foodnutrition.authorId, Literal(row['AuthorId'])))\n",
    "    g.set((review_uri, SDO.author, Literal(row['AuthorName'])))\n",
    "    g.set((review_uri, SDO.reviewBody, Literal(row['Review'])))\n",
    "    g.set((review_uri, SDO.datePublished, Literal(row['DateSubmitted'], datatype=XSD.date)))\n",
    "    g.set((review_uri, SDO.dateModified, Literal(row['DateModified'], datatype=XSD.date)))\n",
    "    \n",
    "    # Add sentiment and confidence\n",
    "    g.set((review_uri, foodnutrition.sentiment, Literal(row['Sentiment'], datatype=XSD.string)))\n",
    "    g.set((review_uri, foodnutrition.confidence, Literal(row['Confidence'], datatype=XSD.float)))\n",
    "\n",
    "    if i == 10: break\n",
    "\n",
    "print(g.serialize('top10Reviews.ttl'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Entity linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ingredient  Count\n",
      "0      bread      6\n",
      "1    chicken      4\n",
      "2     garlic      4\n",
      "3        pie      3\n",
      "4       soup      3\n"
     ]
    }
   ],
   "source": [
    "# Entity linking and ontology alignment â€“ identify and integrate frequently mentioned ingredients by aligning them with existing ontology classes using OWL or SKOS mappings\n",
    "import pandas as pd\n",
    "\n",
    "# Convert ingredient_counts dictionary to a sorted DataFrame\n",
    "top_5_ingredients = sorted(ingredient_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Create DataFrame\n",
    "top_5_df = pd.DataFrame(top_5_ingredients, columns=['Ingredient', 'Count'])\n",
    "\n",
    "# Display DataFrame\n",
    "print(top_5_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ingredient                                 DBpedia_URI  \\\n",
      "0      bread  http://dbpedia.org/resource/Category:Bread   \n",
      "1    chicken                                        None   \n",
      "2     garlic                                        None   \n",
      "3        pie                                        None   \n",
      "4       soup                                        None   \n",
      "\n",
      "                               Wikidata_URI  \n",
      "0      http://www.wikidata.org/entity/Q7802  \n",
      "1       http://www.wikidata.org/entity/Q780  \n",
      "2     http://www.wikidata.org/entity/Q23400  \n",
      "3  http://www.wikidata.org/entity/Q13360264  \n",
      "4     http://www.wikidata.org/entity/Q41415  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# SPARQL endpoints\n",
    "DBPEDIA_SPARQL = \"https://dbpedia.org/sparql\"\n",
    "WIKIDATA_SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "def query_dbpedia(ingredient):\n",
    "    \"\"\"Query DBpedia to find an entity with the given label.\"\"\"\n",
    "    sparql = SPARQLWrapper(DBPEDIA_SPARQL)\n",
    "    query = f\"\"\"\n",
    "    SELECT ?concept WHERE {{\n",
    "      ?concept rdfs:label ?label .\n",
    "      FILTER (LCASE(STR(?label)) = \"{ingredient.lower()}\") .\n",
    "      FILTER (lang(?label) = \"en\")  # Ensure only English labels\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        if results[\"results\"][\"bindings\"]:\n",
    "            return results[\"results\"][\"bindings\"][0][\"concept\"][\"value\"]  # Return first match\n",
    "    except Exception as e:\n",
    "        print(f\"DBpedia Query Error for {ingredient}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def query_wikidata(ingredient):\n",
    "    \"\"\"Query Wikidata to find an entity with the given label.\"\"\"\n",
    "    sparql = SPARQLWrapper(WIKIDATA_SPARQL)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"MyFoodOntologyBot/1.0 (your@email.com)\")  # Required header\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT ?concept WHERE {{\n",
    "      ?concept rdfs:label \"{ingredient}\"@en .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        if results[\"results\"][\"bindings\"]:\n",
    "            return results[\"results\"][\"bindings\"][0][\"concept\"][\"value\"]  # Return first match\n",
    "    except Exception as e:\n",
    "        print(f\"Wikidata Query Error for {ingredient}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Create a new DataFrame to store mappings\n",
    "ontology_mappings = []\n",
    "\n",
    "for index, row in top_5_df.iterrows():\n",
    "    ingredient = row['Ingredient']  # Get ingredient from top_5_df\n",
    "    dbpedia_match = query_dbpedia(ingredient)\n",
    "    wikidata_match = query_wikidata(ingredient)\n",
    "\n",
    "    ontology_mappings.append({\n",
    "        \"Ingredient\": ingredient,\n",
    "        \"DBpedia_URI\": dbpedia_match,\n",
    "        \"Wikidata_URI\": wikidata_match\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "ontology_df = pd.DataFrame(ontology_mappings)\n",
    "\n",
    "# Display results\n",
    "print(ontology_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added bread with Wikidata URI: http://www.wikidata.org/entity/Q7802 and optional mappings.\n",
      "Added chicken with Wikidata URI: http://www.wikidata.org/entity/Q780 and optional mappings.\n",
      "Added garlic with Wikidata URI: http://www.wikidata.org/entity/Q23400 and optional mappings.\n",
      "Added pie with Wikidata URI: http://www.wikidata.org/entity/Q13360264 and optional mappings.\n",
      "Added soup with Wikidata URI: http://www.wikidata.org/entity/Q41415 and optional mappings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N59b6f3c8a6fb4b8b9fab0ccf804b8bef (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdflib import URIRef, Literal\n",
    "from rdflib.namespace import RDF, OWL, SKOS\n",
    "\n",
    "# Define namespaces for your Knowledge Graph\n",
    "wikidata = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "\n",
    "# Example: Add mappings for the top 5 ingredients to the Knowledge Graph\n",
    "for index, row in ontology_df.iterrows():\n",
    "    ingredient_name = row[\"Ingredient\"]\n",
    "    wikidata_uri = row[\"Wikidata_URI\"]\n",
    "    \n",
    "    # Create a unique URI for the ingredient in the KG\n",
    "    ingredient_uri = foodnutrition[ingredient_name.replace(\" \", \"_\")]  # Replace spaces with underscores to form valid URIs\n",
    "    \n",
    "    # Create RDF triples linking the ingredient to its Wikidata URI\n",
    "    if wikidata_uri:\n",
    "        # Add the ingredient as an instance of Ingredient class\n",
    "        g.add((ingredient_uri, RDF.type, foodnutrition.Ingredient))\n",
    "        \n",
    "        # Link to the Wikidata URI using skos:exactMatch\n",
    "        g.add((ingredient_uri, SKOS.exactMatch, URIRef(wikidata_uri)))\n",
    "        \n",
    "        # # Optionally, add alternative names if available \n",
    "        # alt_labels = row.get(\"Alternative_Names\", [])\n",
    "        # for alt_name in alt_labels:\n",
    "        #     g.add((ingredient_uri, SKOS.altLabel, Literal(alt_name)))\n",
    "        \n",
    "        # # Add OWL:sameAs mapping if you want to map it to a more formal ontology (e.g., FOODON)\n",
    "        # # Example, replace this with an actual FOODON URI if available\n",
    "        # foodon_uri = \"http://purl.obolibrary.org/obo/FOODON_03302520\"  # Example FOODON URI\n",
    "        # g.add((ingredient_uri, OWL.sameAs, URIRef(foodon_uri)))\n",
    "        \n",
    "        print(f\"Added {ingredient_name} with Wikidata URI: {wikidata_uri} and optional mappings.\")\n",
    "    else:\n",
    "        print(f\"No Wikidata URI found for {ingredient_name}\")\n",
    "\n",
    "# Optionally, serialize the updated Knowledge Graph with mappings\n",
    "g.serialize(\"updated_foodnutrition_with_wikidata_uris.ttl\", format=\"turtle\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
