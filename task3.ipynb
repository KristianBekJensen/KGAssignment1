{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  csv\n",
    "import  rdflib\n",
    "from    rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, URIRef\n",
    "from    rdflib.namespace import FOAF, DCTERMS, XSD, RDF, SDO, RDFS\n",
    "\n",
    "g = Graph()\n",
    "foodnutrition = Namespace('http://kg-course/foodnutrition/')\n",
    "# Define namespaces\n",
    "g.bind(\"foodnutrition\", foodnutrition)\n",
    "\n",
    "g.parse(\"final.ttl\", format=\"turtle\")\n",
    "print(\"Turtle file loaded. The graph has {} triples.\".format(len(g)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 NLP extract ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean Reviews.txt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the entire file as raw text\n",
    "with open('Reviews_small.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Replace triple quotes and newlines within reviews\n",
    "# Keep newlines that separate proper reviews (i.e., before ReviewId)\n",
    "cleaned_text = re.sub(r'(\"\"\"\"|\\n)(?!(\\d+\\t))', ' ', raw_text)\n",
    "\n",
    "# Now cleaned_text should have each review on a single line, except for proper row separations\n",
    "from io import StringIO\n",
    "\n",
    "# Use StringIO to treat the cleaned text like a file\n",
    "cleaned_file = StringIO(cleaned_text)\n",
    "\n",
    "# Load into pandas DataFrame\n",
    "rev_df = pd.read_csv(cleaned_file, delimiter='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NLP to extract ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Without csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import csv\n",
    "\n",
    "# Load your DataFrame (assuming 'rev_df' is already prepared)\n",
    "# rev_df = pd.read_csv('path_to_your_cleaned_file.csv')  # Uncomment if needed\n",
    "\n",
    "# NER model for food\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\").to(\"cpu\")\n",
    "ner_pipe = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, device=0)  # device=-1 for CPU\n",
    "\n",
    "# Function to convert entities to a list\n",
    "def convert_entities_to_list(text, entities):\n",
    "    ents = []\n",
    "    for ent in entities:\n",
    "        e = {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "        if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
    "            ents[-1][\"end\"] = e[\"end\"]\n",
    "            continue\n",
    "        ents.append(e)\n",
    "    return [text[e[\"start\"]:e[\"end\"]] for e in ents]\n",
    "\n",
    "# Initialize storage for ingredient counts\n",
    "ingredient_counts = defaultdict(int)\n",
    "ingredients_list = []  # Store extracted ingredients for DataFrame\n",
    "\n",
    "# Process each review\n",
    "for idx, row in rev_df.iterrows():\n",
    "    review_text = row.get('Review', '')  # Ensure the column name matches your DataFrame\n",
    "\n",
    "    # Run NER model\n",
    "    ner_entity_results = ner_pipe(review_text, aggregation_strategy=\"simple\")\n",
    "    ingredients = convert_entities_to_list(review_text, ner_entity_results)\n",
    "\n",
    "    # Update ingredient counts\n",
    "    for recipeName in ingredients:\n",
    "        ingredient_counts[recipeName] += 1\n",
    "\n",
    "    # Store extracted ingredients for DataFrame\n",
    "    ingredients_list.append(', '.join(ingredients) if ingredients else 'None')\n",
    "\n",
    "# Add new \"Ingredients\" column to rev_df\n",
    "rev_df['Ingredients'] = ingredients_list\n",
    "\n",
    "# Generate timestamps for unique filenames\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "extracted_file = f\"{current_time}_extracted_ingredientd:\\Chrome downloads\\Description_of_the_Assignment.pdfs.csv\"\n",
    "counted_file = f\"{current_time}_ingredient_counts.csv\"\n",
    "\n",
    "# Save updated DataFrame to CSV (optional)\n",
    "rev_df.to_csv(extracted_file, index=False, encoding='utf-8')\n",
    "print(f\"Updated DataFrame with extracted ingredients saved to {extracted_file}\")\n",
    "\n",
    "# Write ingredient counts to a separate CSV file\n",
    "with open(counted_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Ingredient', 'Count']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for recipeName, count in sorted(ingredient_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        writer.writerow({'Ingredient': recipeName, 'Count': count})\n",
    "\n",
    "print(f\"Ingredient counts saved to {counted_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add extracted ingredients to our KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add ingredients to Review class\n",
    "# Add Review Properties for extracted ingredients\n",
    "extr_ingred = foodnutrition[\"extractedIngredients\"]\n",
    "\n",
    "g.add((extr_ingred, RDFS.domain, SDO.Review))\n",
    "g.add((extr_ingred, RDFS.range, RDFS.Literal))\n",
    "\n",
    "\n",
    "for idx, row in rev_df.iterrows():\n",
    "    review_uri = foodnutrition[f\"review_{row['ReviewId']}\"]  # Unique review identifier\n",
    "    g.add((review_uri, RDF.type, SDO.Review))  # Add review instance\n",
    "\n",
    "    # Iterate over individual ingredients and add separately\n",
    "    for recipeName in row[\"Ingredients\"].split(\", \"):  # Assuming ingredients are comma-separated\n",
    "        g.add((review_uri, extr_ingred, Literal(recipeName)))  # Add each ingredient as a separate triple\n",
    "\n",
    "\n",
    "# Save the updated KG to a file (optional)\n",
    "g.serialize(\"updated_kg.ttl\", format=\"turtle\")\n",
    "\n",
    "print(\"Ingredients added to the Knowledge Graph!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 NLP Sentiment analysis on Reviews.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the entire file as raw text\n",
    "with open('Reviews_small.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "import re\n",
    "\n",
    "# Replace triple quotes and newlines within reviews\n",
    "# Keep newlines that separate proper reviews (i.e., before ReviewId)\n",
    "cleaned_text = re.sub(r'(\"\"\"\"|\\n)(?!(\\d+\\t))', ' ', raw_text)\n",
    "\n",
    "# Now cleaned_text should have each review on a single line, except for proper row separations\n",
    "from io import StringIO\n",
    "\n",
    "# Use StringIO to treat the cleaned text like a file\n",
    "cleaned_file = StringIO(cleaned_text)\n",
    "\n",
    "# Load into pandas DataFrame\n",
    "rev_df = pd.read_csv(cleaned_file, delimiter='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP extract sentiment and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "# Load Reviews.txt (assuming tab-separated values)\n",
    "# reviews_df = pd.read_csv('Reviews_test.txt', delimiter='\\t')\n",
    "reviews_df = rev_df\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline and use cuda device\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=0)\n",
    "# sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"facebook/bart-large-mnli\", device=0) # for neutral sentiment\n",
    "\n",
    "# Handle NaN values in the 'Review' column\n",
    "reviews_df['Review'] = reviews_df['Review'].fillna(\"\")\n",
    "\n",
    "# Apply sentiment analysis to the reviews\n",
    "sentiment_results = sentiment_pipeline(reviews_df['Review'].tolist())\n",
    "\n",
    "# Add sentiment results back to DataFrame\n",
    "reviews_df['Sentiment'] = [result['label'] for result in sentiment_results]\n",
    "reviews_df['Confidence'] = [result['score'] for result in sentiment_results]\n",
    "\n",
    "# Preview the results\n",
    "# print(reviews_df[['ReviewId', 'Review', 'Sentiment', 'Confidence']])\n",
    "\n",
    "# # Save the results to a CSV file\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"{current_time}_extracted_ingredients.csv\"\n",
    "reviews_df[['ReviewId', 'RecipeId', 'Review', 'Sentiment', 'Confidence']].to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data from Reviews to our KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment and confidence properties for the Review class\n",
    "sentiment = foodnutrition[\"sentiment\"]\n",
    "confidence = foodnutrition[\"confidence\"]\n",
    "\n",
    "g.add((sentiment, RDFS.domain, SDO.Review))\n",
    "g.add((sentiment, RDFS.range, RDFS.Literal))\n",
    "\n",
    "g.add((confidence, RDFS.domain, SDO.Review))\n",
    "g.add((confidence, RDFS.range, RDFS.Literal))\n",
    "\n",
    "\n",
    "i = 0\n",
    "# Loop through each review and add it to the KG\n",
    "for index, row in reviews_df.iterrows():\n",
    "    i += 1\n",
    "    \n",
    "    # Create a unique URI for each review\n",
    "    # review_uri = foodnutrition[\"Review/\" + str(i)]\n",
    "    review_uri = foodnutrition[f\"Review/{row['ReviewId']}\"]\n",
    "\n",
    "    # Add the Review type to the graph\n",
    "    g.add((review_uri, RDF.type, SDO.Review))\n",
    "    # Add properties to the Review\n",
    "    g.set((review_uri, foodnutrition.reviewId, Literal(row['ReviewId'])))\n",
    "    g.set((review_uri, foodnutrition.recipeID, Literal(row['RecipeId'])))\n",
    "    g.set((review_uri, foodnutrition.authorId, Literal(row['AuthorId'])))\n",
    "    g.set((review_uri, SDO.author, Literal(row['AuthorName'])))\n",
    "    g.set((review_uri, SDO.reviewBody, Literal(row['Review'])))\n",
    "    g.set((review_uri, SDO.datePublished, Literal(row['DateSubmitted'], datatype=XSD.date)))\n",
    "    g.set((review_uri, SDO.dateModified, Literal(row['DateModified'], datatype=XSD.date)))\n",
    "    \n",
    "    # Add sentiment and confidence\n",
    "    g.set((review_uri, foodnutrition.sentiment, Literal(row['Sentiment'], datatype=XSD.string)))\n",
    "    g.set((review_uri, foodnutrition.confidence, Literal(row['Confidence'], datatype=XSD.float)))\n",
    "\n",
    "    if i == 10: break\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize('finalUnstructAndStruct.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Entity linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity linking and ontology alignment â€“ identify and integrate frequently mentioned ingredients by aligning them with existing ontology classes using OWL or SKOS mappings\n",
    "import pandas as pd\n",
    "\n",
    "# Convert ingredient_counts dictionary to a sorted DataFrame\n",
    "top_5_ingredients = sorted(ingredient_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Create DataFrame\n",
    "top_5_df = pd.DataFrame(top_5_ingredients, columns=['Ingredient', 'Count'])\n",
    "\n",
    "# Display DataFrame\n",
    "print(top_5_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# SPARQL endpoints\n",
    "WIKIDATA_SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "def query_wikidata(ingredient):\n",
    "    \"\"\"Query Wikidata to find an entity with the given label.\"\"\"\n",
    "    sparql = SPARQLWrapper(WIKIDATA_SPARQL)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"MyFoodOntologyBot/1.0 (your@email.com)\")  # Required header\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT ?concept WHERE {{\n",
    "      ?concept rdfs:label \"{ingredient}\"@en .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        if results[\"results\"][\"bindings\"]:\n",
    "            return results[\"results\"][\"bindings\"][0][\"concept\"][\"value\"]  # Return first match\n",
    "    except Exception as e:\n",
    "        print(f\"Wikidata Query Error for {ingredient}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Create a new DataFrame to store mappings\n",
    "ontology_mappings = []\n",
    "\n",
    "for index, row in top_5_df.iterrows():\n",
    "    recipeName = row['Ingredient']  # Get ingredient from top_5_df\n",
    "    wikidata_match = query_wikidata(recipeName)\n",
    "\n",
    "    ontology_mappings.append({\n",
    "        \"Ingredient\": recipeName,\n",
    "        \"Wikidata_URI\": wikidata_match\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "ontology_df = pd.DataFrame(ontology_mappings)\n",
    "\n",
    "# Display results\n",
    "print(ontology_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import URIRef, Literal\n",
    "from rdflib.namespace import RDF, OWL, SKOS\n",
    "\n",
    "# Define namespaces for your Knowledge Graph\n",
    "wikidata = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "\n",
    "# Example: Add mappings for the top 5 ingredients to the Knowledge Graph\n",
    "for index, row in ontology_df.iterrows():\n",
    "    ingredient_name = row[\"Ingredient\"]\n",
    "    wikidata_uri = row[\"Wikidata_URI\"]\n",
    "    \n",
    "    # Create a unique URI for the ingredient in the KG\n",
    "    ingredient_uri = foodnutrition[ingredient_name.replace(\" \", \"_\")]  # Replace spaces with underscores to form valid URIs\n",
    "    \n",
    "    # Create RDF triples linking the ingredient to its Wikidata URI\n",
    "    if wikidata_uri:\n",
    "        # Add the ingredient as an instance of Ingredient class\n",
    "        g.add((ingredient_uri, RDF.type, foodnutrition.Ingredient))\n",
    "        \n",
    "        # Link to the Wikidata URI using skos:exactMatch\n",
    "        g.add((ingredient_uri, SKOS.exactMatch, URIRef(wikidata_uri)))\n",
    "        \n",
    "        print(f\"Added {ingredient_name} with Wikidata URI: {wikidata_uri} and optional mappings.\")\n",
    "    else:\n",
    "        print(f\"No Wikidata URI found for {ingredient_name}\")\n",
    "\n",
    "# Optionally, serialize the updated Knowledge Graph with mappings\n",
    "g.serialize(\"updated_foodnutrition_with_wikidata_uris.ttl\", format=\"turtle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Integrate external KGs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  rdflib\n",
    "from    rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, Literal, URIRef\n",
    "from    rdflib.namespace import FOAF, DCTERMS, XSD, RDF, SDO, RDFS\n",
    "import pyvis\n",
    "from pyvis.network import Network\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# 1. Load the Turtle file into an RDF graph.\n",
    "# Replace 'your_file.ttl' with the path to your Turtle file.\n",
    "g = Graph()\n",
    "\n",
    "g.parse(\"top1000ResNutRec.ttl\", format=\"turtle\")\n",
    "print(\"Turtle file loaded. The graph has {} triples.\".format(len(g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# SPARQL endpoints\n",
    "DBPEDIA_SPARQL = \"https://dbpedia.org/sparql\"\n",
    "WIKIDATA_SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "def query_wikidata(recipeName):\n",
    "    \"\"\"Query Wikidata to find an entity with the given label.\"\"\"\n",
    "    sparql = SPARQLWrapper(WIKIDATA_SPARQL)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"MyFoodOntologyBot/1.0 (your@email.com)\")  # Required header\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT  ?cusine WHERE{{\n",
    "              ?dish wdt:P31 wd:Q746549;\n",
    "              wdt:P2012 ?cusineID;\n",
    "              rdfs:label ?label.\n",
    "            ?cusineID rdfs:label ?cusine.\n",
    "            filter contains(\"{recipeName}\"@en, ?label)\n",
    "    }}\n",
    "            LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        if results[\"results\"][\"bindings\"]:\n",
    "            return results[\"results\"][\"bindings\"][0][\"cusine\"][\"value\"]  # Return first match\n",
    "    except Exception as e:\n",
    "        print(f\"Wikidata Query Error for {recipeName}: {e}\")\n",
    "    return None\n",
    "\n",
    "query = \"\"\"\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX dbd:\t<http://dbpedia.org/datatype/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "\n",
    "SELECT ?name ?recipe\n",
    "WHERE {\n",
    "    ?recipe a schema:Recipe ;\n",
    "      schema:name ?name .\n",
    "}\n",
    "\"\"\"\n",
    "qResult = g.query(query)\n",
    "\n",
    "for row in qResult:\n",
    "    cusine = query_wikidata(row.name)\n",
    "    if cusine is not None:\n",
    "        g.add((row.recipe, SDO.servesCuisine, Literal(cusine)))\n",
    "        print(f\" {row.name}  + {cusine}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize('finalWithEntityLink.ttl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
