{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 NLP extract ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean Reviews.txt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the entire file as raw text\n",
    "with open('Reviews_test.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Replace triple quotes and newlines within reviews\n",
    "# Keep newlines that separate proper reviews (i.e., before ReviewId)\n",
    "cleaned_text = re.sub(r'(\"\"\"\"|\\n)(?!(\\d+\\t))', ' ', raw_text)\n",
    "\n",
    "# Now cleaned_text should have each review on a single line, except for proper row separations\n",
    "from io import StringIO\n",
    "\n",
    "# Use StringIO to treat the cleaned text like a file\n",
    "cleaned_file = StringIO(cleaned_text)\n",
    "\n",
    "# Load into pandas DataFrame\n",
    "rev_df = pd.read_csv(cleaned_file, delimiter='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\Desktop\\school\\cursuri\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ingredients saved to 20250213_181954_extracted_ingredients.csv\n",
      "Ingredient counts saved to 20250213_181954_ingredient_counts.csv\n"
     ]
    }
   ],
   "source": [
    "# #### With csv for testing\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "# from datetime import datetime\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "# import csv\n",
    "\n",
    "# # rev_df = pd.read_csv('path_to_your_cleaned_file.csv')  # Uncomment if you need to reload\n",
    "\n",
    "# # NER model for food\n",
    "# tokenizer_ner = AutoTokenizer.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")\n",
    "# model_ner = AutoModelForTokenClassification.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\").to(\"cuda\")\n",
    "# ner_pipe = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, device=0) # device=-1 for CPU\n",
    "\n",
    "# # Function to convert entities to a list\n",
    "# def convert_entities_to_list(text, entities):\n",
    "#     ents = []\n",
    "#     for ent in entities:\n",
    "#         e = {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "#         if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
    "#             ents[-1][\"end\"] = e[\"end\"]\n",
    "#             continue\n",
    "#         ents.append(e)\n",
    "#     return [text[e[\"start\"]:e[\"end\"]] for e in ents]\n",
    "\n",
    "# # Initialize storage for results\n",
    "# results = []\n",
    "# ingredient_counts = defaultdict(int)\n",
    "\n",
    "# # Process each review\n",
    "# for idx, row in rev_df.iterrows():\n",
    "#     review_text = row.get('Review', '')  # Ensure the column name matches your DataFrame\n",
    "    \n",
    "#     # Run NER model\n",
    "#     ner_entity_results = ner_pipe(review_text, aggregation_strategy=\"simple\")\n",
    "#     ingredients = convert_entities_to_list(review_text, ner_entity_results)\n",
    "\n",
    "#     # Update ingredient counts\n",
    "#     for ingredient in ingredients:\n",
    "#         ingredient_counts[ingredient] += 1\n",
    "\n",
    "#     # Store the results\n",
    "#     results.append({\n",
    "#         'Review': review_text,\n",
    "#         'Extracted Ingredients': ', '.join(ingredients) if ingredients else 'None'\n",
    "#     })\n",
    "\n",
    "# # Generate timestamps for unique filenames\n",
    "# current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# extracted_file = f\"{current_time}_extracted_ingredients.csv\"\n",
    "# counted_file = f\"{current_time}_ingredient_counts.csv\"\n",
    "\n",
    "\n",
    "# # Write extracted ingredients to a CSV file\n",
    "# with open(extracted_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     fieldnames = ['Review', 'Extracted Ingredients']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(results)\n",
    "\n",
    "# print(f\"Extracted ingredients saved to {extracted_file}\")\n",
    "\n",
    "# # Write ingredient counts to a separate CSV file\n",
    "# with open(counted_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     fieldnames = ['Ingredient', 'Count']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "#     for ingredient, count in sorted(ingredient_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "#         writer.writerow({'Ingredient': ingredient, 'Count': count})\n",
    "\n",
    "# print(f\"Ingredient counts saved to {counted_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NLP to extract ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with extracted ingredients saved to 20250213_230834_extracted_ingredients.csv\n",
      "Ingredient counts saved to 20250213_230834_ingredient_counts.csv\n"
     ]
    }
   ],
   "source": [
    "#### Without csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import csv\n",
    "\n",
    "# Load your DataFrame (assuming 'rev_df' is already prepared)\n",
    "# rev_df = pd.read_csv('path_to_your_cleaned_file.csv')  # Uncomment if needed\n",
    "\n",
    "# NER model for food\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\").to(\"cuda\")\n",
    "ner_pipe = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, device=0)  # device=-1 for CPU\n",
    "\n",
    "# Function to convert entities to a list\n",
    "def convert_entities_to_list(text, entities):\n",
    "    ents = []\n",
    "    for ent in entities:\n",
    "        e = {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "        if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
    "            ents[-1][\"end\"] = e[\"end\"]\n",
    "            continue\n",
    "        ents.append(e)\n",
    "    return [text[e[\"start\"]:e[\"end\"]] for e in ents]\n",
    "\n",
    "# Initialize storage for ingredient counts\n",
    "ingredient_counts = defaultdict(int)\n",
    "ingredients_list = []  # Store extracted ingredients for DataFrame\n",
    "\n",
    "# Process each review\n",
    "for idx, row in rev_df.iterrows():\n",
    "    review_text = row.get('Review', '')  # Ensure the column name matches your DataFrame\n",
    "\n",
    "    # Run NER model\n",
    "    ner_entity_results = ner_pipe(review_text, aggregation_strategy=\"simple\")\n",
    "    ingredients = convert_entities_to_list(review_text, ner_entity_results)\n",
    "\n",
    "    # Update ingredient counts\n",
    "    for ingredient in ingredients:\n",
    "        ingredient_counts[ingredient] += 1\n",
    "\n",
    "    # Store extracted ingredients for DataFrame\n",
    "    ingredients_list.append(', '.join(ingredients) if ingredients else 'None')\n",
    "\n",
    "# Add new \"Ingredients\" column to rev_df\n",
    "rev_df['Ingredients'] = ingredients_list\n",
    "\n",
    "# Generate timestamps for unique filenames\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "extracted_file = f\"{current_time}_extracted_ingredients.csv\"\n",
    "counted_file = f\"{current_time}_ingredient_counts.csv\"\n",
    "\n",
    "# Save updated DataFrame to CSV (optional)\n",
    "rev_df.to_csv(extracted_file, index=False, encoding='utf-8')\n",
    "print(f\"Updated DataFrame with extracted ingredients saved to {extracted_file}\")\n",
    "\n",
    "# Write ingredient counts to a separate CSV file\n",
    "with open(counted_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Ingredient', 'Count']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for ingredient, count in sorted(ingredient_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        writer.writerow({'Ingredient': ingredient, 'Count': count})\n",
    "\n",
    "print(f\"Ingredient counts saved to {counted_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add extracted ingredients to our KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients successfully added to the Knowledge Graph!\n"
     ]
    }
   ],
   "source": [
    "## Add ingredients to Review class\n",
    "# Add Review Properties for extracted ingredients\n",
    "extr_ingred = foodnutrition[\"extractedIngredients\"]\n",
    "\n",
    "g.add((extr_ingred, RDFS.domain, SDO.Review))\n",
    "g.add((extr_ingred, RDFS.range, RDFS.Literal))\n",
    "\n",
    "\n",
    "for idx, row in rev_df.iterrows():\n",
    "    review_uri = foodnutrition[f\"review_{row['ReviewId']}\"]  # Unique review identifier\n",
    "    g.add((review_uri, RDF.type, SDO.Review))  # Add review instance\n",
    "\n",
    "    # Iterate over individual ingredients and add separately\n",
    "    for ingredient in row[\"Ingredients\"].split(\", \"):  # Assuming ingredients are comma-separated\n",
    "        g.add((review_uri, extr_ingred, Literal(ingredient)))  # Add each ingredient as a separate triple\n",
    "\n",
    "\n",
    "# Save the updated KG to a file (optional)\n",
    "g.serialize(\"updated_kg.ttl\", format=\"turtle\")\n",
    "\n",
    "print(\"Ingredients added to the Knowledge Graph!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
